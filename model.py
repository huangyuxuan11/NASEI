import torch
import torch.nn as nn
from operations import *
from torch.autograd import Variable
from utils import drop_path
import torch.nn.functional as F


class Cell(nn.Module):

  def __init__(self, genotype, C_prev_prev, C_prev, C, reduction, reduction_prev):
    super(Cell, self).__init__()
    print(C_prev_prev, C_prev, C)

    if reduction_prev:
      self.preprocess0 = FactorizedReduce(C_prev_prev, C)
    else:
      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0)
    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0)
    
    if reduction:
      op_names, indices = zip(*genotype.reduce)
      concat = genotype.reduce_concat
    else:
      op_names, indices = zip(*genotype.normal)
      concat = genotype.normal_concat
    self._compile(C, op_names, indices, concat, reduction)

  def _compile(self, C, op_names, indices, concat, reduction):
    assert len(op_names) == len(indices)
    self._steps = len(op_names) // 2
    self._concat = concat
    self.multiplier = len(concat)

    self._ops = nn.ModuleList()
    for name, index in zip(op_names, indices):
      stride = 2 if reduction and index < 2 else 1
      op = OPS[name](C, stride, True)
      self._ops += [op]
    self._indices = indices

  def forward(self, s0, s1, drop_prob):
    s0 = self.preprocess0(s0)
    s1 = self.preprocess1(s1)

    states = [s0, s1]
    for i in range(self._steps):
      h1 = states[self._indices[2*i]]
      h2 = states[self._indices[2*i+1]]
      op1 = self._ops[2*i]
      op2 = self._ops[2*i+1]
      h1 = op1(h1)
      h2 = op2(h2)
      if self.training and drop_prob > 0.:
        if not isinstance(op1, Identity):
          h1 = drop_path(h1, drop_prob)
        if not isinstance(op2, Identity):
          h2 = drop_path(h2, drop_prob)
      s = h1 + h2
      states += [s]
    return torch.cat([states[i] for i in self._concat], dim=1)


class AuxiliaryHeadCIFAR(nn.Module):

  def __init__(self, C, num_classes):
    """assuming input size 8x8"""
    super(AuxiliaryHeadCIFAR, self).__init__()
    self.features = nn.Sequential(
      nn.ReLU(inplace=True),
      nn.AvgPool2d(5, stride=3, padding=0, count_include_pad=False), # image size = 2 x 2
      nn.Conv2d(C, 128, 1, bias=False),
      nn.BatchNorm2d(128),
      nn.ReLU(inplace=True),
      nn.Conv2d(128, 768, 2, bias=False),
      nn.BatchNorm2d(768),
      nn.ReLU(inplace=True)
    )
    self.classifier = nn.Linear(768, num_classes)

  def forward(self, x):
    x = self.features(x)
    x = self.classifier(x.view(x.size(0),-1))
    return x


class AuxiliaryHeadImageNet(nn.Module):

  def __init__(self, C, num_classes):
    """assuming input size 14x14"""
    super(AuxiliaryHeadImageNet, self).__init__()
    self.features = nn.Sequential(
      nn.ReLU(inplace=True),
      nn.AvgPool2d(5, stride=2, padding=0, count_include_pad=False),
      nn.Conv2d(C, 128, 1, bias=False),
      nn.BatchNorm2d(128),
      nn.ReLU(inplace=True),
      nn.Conv2d(128, 768, 2, bias=False),
      # NOTE: This batchnorm was omitted in my earlier implementation due to a typo.
      # Commenting it out for consistency with the experiments in the paper.
      # nn.BatchNorm2d(768),
      nn.ReLU(inplace=True)
    )
    self.classifier = nn.Linear(768, num_classes)

  def forward(self, x):
    x = self.features(x)
    x = self.classifier(x.view(x.size(0),-1))
    return x


class NetworkCIFAR(nn.Module):

  def __init__(self, C, num_classes, layers, auxiliary, genotype):
    super(NetworkCIFAR, self).__init__()
    self._layers = layers
    self._auxiliary = auxiliary

    stem_multiplier = 3
    C_curr = stem_multiplier*C
    self.stem = nn.Sequential(
      nn.Conv2d(1, C_curr, 3, padding=1, bias=False),
      nn.BatchNorm2d(C_curr)
    )
    
    C_prev_prev, C_prev, C_curr = C_curr, C_curr, C
    self.cells = nn.ModuleList()
    reduction_prev = False
    for i in range(layers):
      if i in [1]:
        C_curr *= 2
        reduction = True
      else:
        reduction = False
      cell = Cell(genotype, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)
      reduction_prev = reduction
      self.cells += [cell]
      C_prev_prev, C_prev = C_prev, cell.multiplier*C_curr
      if i == 2*layers//3:
        C_to_auxiliary = C_prev

    if auxiliary:
      self.auxiliary_head = AuxiliaryHeadCIFAR(C_to_auxiliary, num_classes)
    self.global_pooling = nn.AdaptiveAvgPool2d(1)
    self.classifier = nn.Linear(C_prev, num_classes)

  def forward(self, input):
    logits_aux = None
    s0 = s1 = self.stem(input)
    for i, cell in enumerate(self.cells):
      s0, s1 = s1, cell(s0, s1, self.drop_path_prob)
      if i == 2*self._layers//3:
        if self._auxiliary and self.training:
          logits_aux = self.auxiliary_head(s1)
    out = self.global_pooling(s1)
    logits = self.classifier(out.view(out.size(0),-1))
    return logits, logits_aux


class NetworkImageNet(nn.Module):

  def __init__(self, C, num_classes, layers, auxiliary, genotype):
    super(NetworkImageNet, self).__init__()
    self._layers = layers
    self._auxiliary = auxiliary

    self.stem0 = nn.Sequential(
      nn.Conv2d(3, C // 2, kernel_size=3, stride=2, padding=1, bias=False),
      nn.BatchNorm2d(C // 2),
      nn.ReLU(inplace=True),
      nn.Conv2d(C // 2, C, 3, stride=2, padding=1, bias=False),
      nn.BatchNorm2d(C),
    )

    self.stem1 = nn.Sequential(
      nn.ReLU(inplace=True),
      nn.Conv2d(C, C, 3, stride=2, padding=1, bias=False),
      nn.BatchNorm2d(C),
    )

    C_prev_prev, C_prev, C_curr = C, C, C

    self.cells = nn.ModuleList()
    reduction_prev = True
    for i in range(layers):
      if i in [1]:
        C_curr *= 2
        reduction = True
      else:
        reduction = False
      cell = Cell(genotype, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)
      reduction_prev = reduction
      self.cells += [cell]
      C_prev_prev, C_prev = C_prev, cell.multiplier * C_curr
      if i == 2 * layers // 3:
        C_to_auxiliary = C_prev

    if auxiliary:
      self.auxiliary_head = AuxiliaryHeadImageNet(C_to_auxiliary, num_classes)
    self.global_pooling = nn.AvgPool2d(7)
    self.classifier = nn.Linear(C_prev, num_classes)

  def forward(self, input):
    logits_aux = None
    s0 = self.stem0(input)
    s1 = self.stem1(s0)
    for i, cell in enumerate(self.cells):
      s0, s1 = s1, cell(s0, s1, self.drop_path_prob)
      if i == 2 * self._layers // 3:
        if self._auxiliary and self.training:
          logits_aux = self.auxiliary_head(s1)
    out = self.global_pooling(s1)
    logits = self.classifier(out.view(out.size(0), -1))
    return logits, logits_aux

class VGG(nn.Module):
    def __init__(self, features, num_classes=8, init_weights=False):
      super(VGG, self).__init__()
      self.features = features
      self.classifier = nn.Sequential(
        nn.Dropout(p=0.5),
        nn.Linear(512 * 3, 2048),
        nn.ReLU(True),
        nn.Dropout(p=0.5),
        nn.Linear(2048, 2048),
        nn.ReLU(True),
        nn.Linear(2048, num_classes)
      )
      if init_weights:
        self._initialize_weights()

    def forward(self, x):
      # N x 3 x 224 x 224
      x = self.features(x)
      # N x 512 x 7 x 7
      x = torch.flatten(x, start_dim=1)
      # N x 512*7*7
      x = self.classifier(x)
      return x

    def _initialize_weights(self):
      for m in self.modules():
        if isinstance(m, nn.Conv2d):
          # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
          nn.init.xavier_uniform_(m.weight)
          if m.bias is not None:
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
          nn.init.xavier_uniform_(m.weight)
          # nn.init.normal_(m.weight, 0, 0.01)
          nn.init.constant_(m.bias, 0)


def make_features(cfg: list):
  layers = []
  in_channels = 1
  for v in cfg:
    if v == "M":
      layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
    else:
      conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)
      layers += [conv2d, nn.ReLU(True)]
      in_channels = v
  return nn.Sequential(*layers)


cfgs = {
  'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
  'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
  'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
  'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],
}


def vgg(model_name="vgg16", **kwargs):
  try:
    cfg = cfgs[model_name]
  except:
    print("Warning: model number {} not in cfgs dict!".format(model_name))
    exit(-1)
  model = VGG(make_features(cfg), **kwargs)
  return model


# resnet

class BasicBlock(nn.Module):
  expansion = 1

  def __init__(self, in_channel, out_channel, stride=1, downsample=None):
    super(BasicBlock, self).__init__()
    self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,
                           kernel_size=3, stride=stride, padding=1, bias=False)
    self.bn1 = nn.BatchNorm2d(out_channel)
    self.relu = nn.ReLU()
    self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,
                           kernel_size=3, stride=1, padding=1, bias=False)
    self.bn2 = nn.BatchNorm2d(out_channel)
    self.downsample = downsample

  def forward(self, x):
    identity = x
    if self.downsample is not None:
      identity = self.downsample(x)

    out = self.conv1(x)
    out = self.bn1(out)
    out = self.relu(out)

    out = self.conv2(out)
    out = self.bn2(out)

    out += identity
    out = self.relu(out)

    return out


class Bottleneck(nn.Module):
  expansion = 4

  def __init__(self, in_channel, out_channel, stride=1, downsample=None):
    super(Bottleneck, self).__init__()
    self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,
                           kernel_size=1, stride=1, bias=False)  # squeeze channels
    self.bn1 = nn.BatchNorm2d(out_channel)
    # -----------------------------------------
    self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,
                           kernel_size=3, stride=stride, bias=False, padding=1)
    self.bn2 = nn.BatchNorm2d(out_channel)
    # -----------------------------------------
    self.conv3 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel * self.expansion,
                           kernel_size=1, stride=1, bias=False)  # unsqueeze channels
    self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)
    self.relu = nn.ReLU(inplace=True)
    self.downsample = downsample

  def forward(self, x):
    identity = x
    if self.downsample is not None:
      identity = self.downsample(x)

    out = self.conv1(x)
    out = self.bn1(out)
    out = self.relu(out)

    out = self.conv2(out)
    out = self.bn2(out)
    out = self.relu(out)

    out = self.conv3(out)
    out = self.bn3(out)

    out += identity
    out = self.relu(out)

    return out


class ResNet(nn.Module):

  def __init__(self, block, blocks_num, num_classes=1000, include_top=True):
    super(ResNet, self).__init__()
    self.include_top = include_top
    self.in_channel = 64

    self.conv1 = nn.Conv2d(1, self.in_channel, kernel_size=7, stride=2,
                           padding=3, bias=False)
    self.bn1 = nn.BatchNorm2d(self.in_channel)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, blocks_num[0])
    self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)
    self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)
    self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)
    if self.include_top:
      self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # output size = (1, 1)
      self.fc = nn.Linear(512 * block.expansion, num_classes)

    for m in self.modules():
      if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

  def _make_layer(self, block, channel, block_num, stride=1):
    downsample = None
    if stride != 1 or self.in_channel != channel * block.expansion:
      downsample = nn.Sequential(
        nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),
        nn.BatchNorm2d(channel * block.expansion))

    layers = []
    layers.append(block(self.in_channel, channel, downsample=downsample, stride=stride))
    self.in_channel = channel * block.expansion

    for _ in range(1, block_num):
      layers.append(block(self.in_channel, channel))

    return nn.Sequential(*layers)

  def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.maxpool(x)

    x = self.layer1(x)
    x = self.layer2(x)
    x = self.layer3(x)
    x = self.layer4(x)

    if self.include_top:
      x = self.avgpool(x)
      x = torch.flatten(x, 1)
      x = self.fc(x)

    return x


def resnet34(num_classes=8, include_top=True):
  return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)


def resnet50(num_classes=1000, include_top=True):
  # https://download.pytorch.org/models/resnet50-19c8e357.pth
  return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)


def resnet101(num_classes=8, include_top=True):
  return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)


# class AlexNet(nn.Module):
#   def __init__(self, num_classes=12, init_weights=False):
#     super(AlexNet, self).__init__()
#     self.features = nn.Sequential(
#       nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2),  # input[3, 224, 224]  output[48, 55, 55]
#       nn.ReLU(inplace=True),
#       nn.MaxPool2d(kernel_size=3, stride=2),  # output[48, 27, 27]
#       nn.Conv2d(48, 128, kernel_size=5, padding=2),  # output[128, 27, 27]
#       nn.ReLU(inplace=True),
#       nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 13, 13]
#       nn.Conv2d(128, 192, kernel_size=3, padding=1),  # output[192, 13, 13]
#       nn.ReLU(inplace=True),
#       nn.Conv2d(192, 192, kernel_size=3, padding=1),  # output[192, 13, 13]
#       nn.ReLU(inplace=True),
#       nn.Conv2d(192, 128, kernel_size=3, padding=1),  # output[128, 13, 13]
#       nn.ReLU(inplace=True),
#       nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 6, 6]
#     )
#     self.classifier = nn.Sequential(
#       nn.Dropout(p=0.5),
#       nn.Linear(128 * 6 * 6, 2048),
#       nn.ReLU(inplace=True),
#       nn.Dropout(p=0.5),
#       nn.Linear(2048, 2048),
#       nn.ReLU(inplace=True),
#       nn.Linear(2048, num_classes),
#     )
#     if init_weights:
#       self._initialize_weights()
#
#   def forward(self, x):
#     x = self.features(x)
#     x = torch.flatten(x, start_dim=1)
#     x = self.classifier(x)
#     return x
#
#   def _initialize_weights(self):
#     for m in self.modules():
#       if isinstance(m, nn.Conv2d):
#         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
#         if m.bias is not None:
#           nn.init.constant_(m.bias, 0)
#       elif isinstance(m, nn.Linear):
#         nn.init.normal_(m.weight, 0, 0.01)
#         nn.init.constant_(m.bias, 0)


class AlexNet(nn.Module):
  def __init__(self, num_classes=12, init_weights=False):
    super(AlexNet, self).__init__()
    self.features = nn.Sequential(
      nn.Conv2d(1, 48, kernel_size=8, stride=1, padding=1),  # input[2, 100, 60]  output[48, 95, 55]
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=3, stride=2),  # output[48, 47, 27]
      nn.Conv2d(48, 128, kernel_size=5, padding=2),  # output[128, 47, 27]
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 23, 13]
      nn.Conv2d(128, 192, kernel_size=3, padding=1),  # output[192, 23, 13]
      nn.ReLU(inplace=True),
      nn.Conv2d(192, 192, kernel_size=3, padding=1),  # output[192, 23, 13]
      nn.ReLU(inplace=True),
      nn.Conv2d(192, 128, kernel_size=3, padding=1),  # output[128, 23, 13]
      nn.ReLU(inplace=True),
      nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 11, 6]
    )
    self.classifier = nn.Sequential(
      nn.Dropout(p=0.5),
      nn.Linear(128 * 11 * 6, 4096),
      nn.ReLU(inplace=True),
      nn.Dropout(p=0.5),
      nn.Linear(4096, 2048),
      nn.ReLU(inplace=True),
      nn.Linear(2048, num_classes),
    )
    if init_weights:
      self._initialize_weights()

  def forward(self, x):
    x = self.features(x)
    x = torch.flatten(x, start_dim=1)
    x = self.classifier(x)
    return x

  def _initialize_weights(self):
    for m in self.modules():
      if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        if m.bias is not None:
          nn.init.constant_(m.bias, 0)
      elif isinstance(m, nn.Linear):
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.constant_(m.bias, 0)


class GoogLeNet(nn.Module):
  def __init__(self, num_classes=8, aux_logits=True, init_weights=False):
    super(GoogLeNet, self).__init__()
    self.aux_logits = aux_logits

    self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3)
    self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)

    self.conv2 = BasicConv2d(64, 64, kernel_size=1)
    self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1)
    self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)

    self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)
    self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)
    self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)

    self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)
    self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)
    self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)
    self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)
    self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)
    self.maxpool4 = nn.MaxPool2d(3, stride=2, ceil_mode=True)

    self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)
    self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)

    if self.aux_logits:
      self.aux1 = InceptionAux(512, num_classes)
      self.aux2 = InceptionAux(528, num_classes)

    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.dropout = nn.Dropout(0.4)
    self.fc = nn.Linear(1024, num_classes)
    if init_weights:
      self._initialize_weights()

  def forward(self, x):
    # N x 3 x 224 x 224
    x = self.conv1(x)
    # N x 64 x 112 x 112
    x = self.maxpool1(x)
    # N x 64 x 56 x 56
    x = self.conv2(x)
    # N x 64 x 56 x 56
    x = self.conv3(x)
    # N x 192 x 56 x 56
    x = self.maxpool2(x)

    # N x 192 x 28 x 28
    x = self.inception3a(x)
    # N x 256 x 28 x 28
    x = self.inception3b(x)
    # N x 480 x 28 x 28
    x = self.maxpool3(x)
    # N x 480 x 14 x 14
    x = self.inception4a(x)
    # N x 512 x 14 x 14
    if self.training and self.aux_logits:  # eval model lose this layer
      aux1 = self.aux1(x)

    x = self.inception4b(x)
    # N x 512 x 14 x 14
    x = self.inception4c(x)
    # N x 512 x 14 x 14
    x = self.inception4d(x)
    # N x 528 x 14 x 14
    if self.training and self.aux_logits:  # eval model lose this layer
      aux2 = self.aux2(x)

    x = self.inception4e(x)
    # N x 832 x 14 x 14
    x = self.maxpool4(x)
    # N x 832 x 7 x 7
    x = self.inception5a(x)
    # N x 832 x 7 x 7
    x = self.inception5b(x)
    # N x 1024 x 7 x 7

    x = self.avgpool(x)
    # N x 1024 x 1 x 1
    x = torch.flatten(x, 1)
    # N x 1024
    x = self.dropout(x)
    x = self.fc(x)
    # N x 1000 (num_classes)
    if self.training and self.aux_logits:  # eval model lose this layer
      return x, aux2, aux1
    return x

  def _initialize_weights(self):
    for m in self.modules():
      if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        if m.bias is not None:
          nn.init.constant_(m.bias, 0)
      elif isinstance(m, nn.Linear):
        nn.init.normal_(m.weight, 0, 0.01)
        nn.init.constant_(m.bias, 0)


class Inception(nn.Module):
  def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):
    super(Inception, self).__init__()

    self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)

    self.branch2 = nn.Sequential(
      BasicConv2d(in_channels, ch3x3red, kernel_size=1),
      BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)  # 保证输出大小等于输入大小
    )

    self.branch3 = nn.Sequential(
      BasicConv2d(in_channels, ch5x5red, kernel_size=1),
      BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)  # 保证输出大小等于输入大小
    )

    self.branch4 = nn.Sequential(
      nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
      BasicConv2d(in_channels, pool_proj, kernel_size=1)
    )

  def forward(self, x):
    branch1 = self.branch1(x)
    branch2 = self.branch2(x)
    branch3 = self.branch3(x)
    branch4 = self.branch4(x)

    outputs = [branch1, branch2, branch3, branch4]
    return torch.cat(outputs, 1)


class InceptionAux(nn.Module):
  def __init__(self, in_channels, num_classes):
    super(InceptionAux, self).__init__()
    self.averagePool = nn.AvgPool2d(kernel_size=5, stride=3)
    self.conv = BasicConv2d(in_channels, 128, kernel_size=1)  # output[batch, 128, 4, 4]

    self.fc1 = nn.Linear(2048, 1024)
    self.fc2 = nn.Linear(1024, num_classes)

  def forward(self, x):
    # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14
    x = self.averagePool(x)
    # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4
    x = self.conv(x)
    # N x 128 x 4 x 4
    x = torch.flatten(x, 1)
    x = F.dropout(x, 0.5, training=self.training)
    # N x 2048
    x = F.relu(self.fc1(x), inplace=True)
    x = F.dropout(x, 0.5, training=self.training)
    # N x 1024
    x = self.fc2(x)
    # N x num_classes
    return x


class BasicConv2d(nn.Module):
  def __init__(self, in_channels, out_channels, **kwargs):
    super(BasicConv2d, self).__init__()
    self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)
    self.relu = nn.ReLU(inplace=True)

  def forward(self, x):
    x = self.conv(x)
    x = self.relu(x)
    return x


class CNN(nn.Module):
  def __init__(self):
    super(CNN, self).__init__()
    self.conv1 = nn.Sequential(
      nn.Conv2d(1, 64, 7, 1, 3),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size=2),
    )
    self.conv2 = nn.Sequential(
      nn.Conv2d(64, 64, 3, 1, 1),
      nn.ReLU(),
      nn.MaxPool2d(2),
    )
    self.conv3 = nn.Sequential(
      nn.Conv2d(64, 64, 3, 1, 1),
      nn.ReLU(),
      nn.MaxPool2d(2),
    )
    self.conv4 = nn.Sequential(
      nn.Conv2d(64, 32, 3, 1, 1),
      nn.ReLU(),
      nn.MaxPool2d(2),
    )
    self.out1 = nn.Sequential(
      nn.Linear(576,61440),
      nn.ReLU(),
    )
    self.out2 = nn.Sequential(
      nn.Linear(61440, 200),
      nn.ReLU(),
    )
    self.out3 = nn.Sequential(
      nn.Linear(200, 30),
    )

  def forward(self, x):
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.conv3(x)
    x = self.conv4(x)
    x = x.view(x.size(0), -1)  # flatten the output of conv2 to (batch_size, 32 * 7 * 7)
    x = self.out1(x)
    x = self.out2(x)
    output = self.out3(x)
    return output, x  # return x for visualization


class LeNet(nn.Module):
  # 一般在__init__中定义网络需要的操作算子，比如卷积、全连接算子等等
  def __init__(self):
    super(LeNet, self).__init__()
    # Conv2d的第一个参数是输入的channel数量，第二个是输出的channel数量，第三个是kernel size
    self.conv1 = nn.Conv2d(1, 6, 5)
    self.conv2 = nn.Conv2d(6, 32, 5)
    # 由于上一层有16个channel输出，每个feature map大小为5*5，所以全连接层的输入是16*5*5
    self.fc1 = nn.Linear(8448, 120)
    # self.fc1 = nn.Linear(89888, 120)
    self.fc2 = nn.Linear(120, 84)
    # 最终有10类，所以最后一个全连接层输出数量是10
    self.fc3 = nn.Linear(84, 30)
    self.pool = nn.MaxPool2d(2, 2)

  # forward这个函数定义了前向传播的运算，只需要像写普通的python算数运算那样就可以了
  def forward(self, x):
    x = F.relu(self.conv1(x))
    x = self.pool(x)
    x = F.relu(self.conv2(x))
    x = self.pool(x)
    # 下面这步把二维特征图变为一维，这样全连接层才能处理
    x = x.view(x.size(0), -1)  # 16*5*5
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    output = self.fc3(x)
    return output, x


